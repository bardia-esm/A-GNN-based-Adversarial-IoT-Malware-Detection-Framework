{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from IPython.display import clear_output\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "from time import time\n",
    "import re\n",
    "\n",
    "from utils import read_graph_data, read_pickle, plot_histogram, process_graph_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data, Dataset\n",
    "class CFG(Dataset):\n",
    "    \n",
    "    def __init__(self, graph_addr_list, root, label_transformer, sample_desc = None, transform=None, pre_transform=None):\n",
    "        self.graph_addr_list = graph_addr_list\n",
    "        self.label_transformer = label_transformer\n",
    "        self.sample_desc = sample_desc\n",
    "        super(CFG, self).__init__(root, transform, pre_transform)\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return [f'data_{file_idx}.pt' for file_idx in range(len(self.graph_addr_list))]\n",
    "\n",
    "    def process(self):\n",
    "        graph_data_mapping = []\n",
    "        one_hot_transform = T.OneHotDegree(max_degree = 187, cat = False, in_degree = True)\n",
    "\n",
    "        for graph_idx, graph_addr in enumerate(self.graph_addr_list):\n",
    "            try:\n",
    "                graph_data = read_graph_data(graph_addr)\n",
    "\n",
    "                raw_edges = graph_data['edge_list']\n",
    "                raw_nodes = list(graph_data['node_dict'].keys())\n",
    "\n",
    "                if 'benign' in graph_addr:\n",
    "                    y = 'benign'\n",
    "                elif 'tsunami' in graph_addr:\n",
    "                    y = 'tsunami'\n",
    "                elif 'mirai' in graph_addr:\n",
    "                    y = 'mirai'\n",
    "                elif 'gafgyt' in graph_addr:\n",
    "                    y = 'gafgyt'\n",
    "                    \n",
    "\n",
    "                unique_node_idx_counter = 0\n",
    "                node_mapping = {}\n",
    "                edges = [[], []]\n",
    "\n",
    "                for node in raw_nodes:\n",
    "                    node_mapping[str(node)] = unique_node_idx_counter\n",
    "                    unique_node_idx_counter += 1\n",
    "\n",
    "                for edge in raw_edges:\n",
    "                    edges[0].append(node_mapping[str(edge[0])])\n",
    "                    edges[1].append(node_mapping[str(edge[1])])\n",
    "\n",
    "                edge_idx = torch.tensor(edges, dtype=torch.long)\n",
    "                y = torch.tensor(self.label_transformer.transform([y]), dtype=torch.long)\n",
    "                x = None\n",
    "                data = Data(x=x, edge_index=edge_idx, y=y)\n",
    "                data = one_hot_transform(data)\n",
    "                \n",
    "                if self.pre_filter is not None and not self.pre_filter(data):\n",
    "                    continue\n",
    "\n",
    "                if self.pre_transform is not None:\n",
    "                    data = self.pre_transform(data)\n",
    "\n",
    "                torch.save(data, osp.join(self.processed_dir, 'data_{}.pt'.format(graph_idx)))\n",
    "            except Exception as e:\n",
    "                print(graph_addr, e)\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.processed_file_names)\n",
    "\n",
    "    def get(self, idx):\n",
    "        data = torch.load(osp.join(self.processed_dir, 'data_{}.pt'.format(idx)))\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benign_train_graph_list = glob(\"datasets/normal_dataset/benign/train/*\")\n",
    "benign_test_graph_list = glob(\"datasets/normal_dataset/benign/test/*\")\n",
    "print('benign', len(benign_train_graph_list), len(benign_test_graph_list))\n",
    "\n",
    "tsunami_train_graph_list = glob(\"datasets/normal_dataset/tsunami/train/*\")\n",
    "tsunami_test_graph_list = glob(\"datasets/normal_dataset/tsunami/test/*\")\n",
    "print('tsunami', len(tsunami_train_graph_list), len(tsunami_test_graph_list))\n",
    "\n",
    "mirai_train_graph_list = glob(\"datasets/normal_dataset/mirai/train/*\")\n",
    "mirai_test_graph_list = glob(\"datasets/normal_dataset/mirai/test/*\")\n",
    "print('mirai', len(mirai_train_graph_list), len(mirai_test_graph_list))\n",
    "\n",
    "gafgyt_train_graph_list = glob(\"datasets/normal_dataset/gafgyt/train/*\")\n",
    "gafgyt_test_graph_list = glob(\"datasets/normal_dataset/gafgyt/test/*\")\n",
    "print('gafgyt', len(gafgyt_train_graph_list), len(gafgyt_test_graph_list))\n",
    "\n",
    "all_train_graphs_list = benign_train_graph_list + tsunami_train_graph_list + gafgyt_train_graph_list + mirai_train_graph_list\n",
    "all_test_graphs_list = benign_test_graph_list + tsunami_test_graph_list + gafgyt_test_graph_list + mirai_test_graph_list\n",
    "\n",
    "print(f'Train: {len(all_train_graphs_list)}, Test: {len(all_test_graphs_list)}')\n",
    "\n",
    "\n",
    "for i in benign_train_graph_list + benign_test_graph_list + tsunami_train_graph_list + tsunami_test_graph_list + gafgyt_train_graph_list + gafgyt_test_graph_list + mirai_train_graph_list + mirai_test_graph_list:\n",
    "    if not os.path.exists(i):\n",
    "        print(i)\n",
    "\n",
    "all_train_graphs_list = benign_train_graph_list + tsunami_train_graph_list + gafgyt_train_graph_list + mirai_train_graph_list\n",
    "all_test_graphs_list = benign_test_graph_list + tsunami_test_graph_list + gafgyt_test_graph_list + mirai_test_graph_list\n",
    "\n",
    "print(f'Train: {len(all_train_graphs_list)}, Test: {len(all_test_graphs_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp \n",
    "from torch_geometric.data import Data, Dataset, Batch\n",
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.nn import Linear, Sequential, ReLU\n",
    "from torch_scatter import scatter_mean\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GraphConv, GCNConv, SAGEConv, GATConv, GINConv, global_mean_pool, global_add_pool\n",
    "from torch_geometric.utils import remove_self_loops, add_self_loops\n",
    "from torch_geometric.data import Data\n",
    "import torch_geometric.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_classes = ['benign', 'tsunami', 'mirai', 'gafgyt']\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(selected_classes)\n",
    "\n",
    "print('Label Encdoer: ', le.transform(['benign', 'tsunami', 'mirai', 'gafgyt']))\n",
    "\n",
    "\n",
    "train_dataset = CFG(graph_addr_list = all_train_graphs_list, root='data/train', label_transformer = le)\n",
    "print(f'Number of training graphs: {train_dataset.len()}')\n",
    "\n",
    "test_dataset = CFG(graph_addr_list = all_test_graphs_list, root='data/test', label_transformer = le)\n",
    "print(f'Number of test graphs: {test_dataset.len()}')\n",
    "\n",
    "adversarial_dir = \"datasets/adversarial_dataset\"\n",
    "adversarial_dataset = {'adversarial_benign': [], 'adversarial_gafgyt': [], 'adversarial_mirai': [], 'adversarial_tsunami': []}\n",
    "for i in glob(adversarial_dir + '/*'):\n",
    "    adversarial_graph_data = read_graph_data(i)\n",
    "    if re.search(r\"{}_(max|median|min)\".format('benign'), i):\n",
    "        adversarial_dataset['adversarial_benign'].append(T.OneHotDegree(max_degree = 187, cat = False, in_degree = True)(process_graph_data(adversarial_graph_data, 'adversarial', le)))\n",
    "    elif re.search(r\"{}_(max|median|min)\".format('gafgyt'), i):\n",
    "        adversarial_dataset['adversarial_gafgyt'].append(T.OneHotDegree(max_degree = 187, cat = False, in_degree = True)(process_graph_data(adversarial_graph_data, 'adversarial', le)))\n",
    "    elif re.search(r\"{}_(max|median|min)\".format('mirai'), i):\n",
    "        adversarial_dataset['adversarial_mirai'].append(T.OneHotDegree(max_degree = 187, cat = False, in_degree = True)(process_graph_data(adversarial_graph_data, 'adversarial', le)))\n",
    "    elif re.search(r\"{}_(max|median|min)\".format('tsunami'), i):\n",
    "        adversarial_dataset['adversarial_tsunami'].append(T.OneHotDegree(max_degree = 187, cat = False, in_degree = True)(process_graph_data(adversarial_graph_data, 'adversarial', le)))\n",
    "\n",
    "print(f\"Number of adversarial graphs: {len(adversarial_dataset['adversarial_benign']) + len(adversarial_dataset['adversarial_gafgyt']) + len(adversarial_dataset['adversarial_mirai']) + len(adversarial_dataset['adversarial_tsunami'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(train_dataset, test_dataset, adversarial_dataset, batch_size):\n",
    "    dataset_dict = {\n",
    "        'normal': {'train': DataLoader(train_dataset, batch_size = batch_size, shuffle = True), 'test': DataLoader(test_dataset, batch_size = batch_size, shuffle = False)},\n",
    "        'adversarial': {adv_k: DataLoader(adv_v, batch_size = batch_size, shuffle = False) for adv_k, adv_v in adversarial_dataset.items()}\n",
    "    }\n",
    "\n",
    "    for k, v in dataset_dict.items():\n",
    "        if k == 'adversarial':\n",
    "            for temp_k, temp_v in v.items():\n",
    "                print(f'Number of {temp_k} Samples: {len(temp_v.dataset)}')\n",
    "        else:\n",
    "            print(f\"Number of {k} samples in train-set: {len(v['train'].dataset)}\", f\"Number of {k} samples in test-set: {len(v['test'].dataset)}\")\n",
    "\n",
    "    return dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Detector(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Detector, self).__init__()\n",
    "        self.config = config\n",
    "\n",
    "        if self.config['conv_layer_type'].__name__ == 'GCNConv':\n",
    "            self.graphConv_layer_list = torch.nn.ModuleList([self.config['conv_layer_type'](*layer, bias = False) for layer in self.config['layer_size_list']])\n",
    "        elif self.config['conv_layer_type' ].__name__ == 'GINConv':\n",
    "            self.graphConv_layer_list = torch.nn.ModuleList()\n",
    "            for layer in self.config['layer_size_list']:\n",
    "                in_mlp = Sequential(\n",
    "                    Linear(layer[0], layer[1], bias = False), \n",
    "                    torch.nn.BatchNorm1d(layer[1]), \n",
    "                    ReLU(), \n",
    "                    Linear(layer[1], layer[1], bias = False))\n",
    "                self.graphConv_layer_list.append(self.config['conv_layer_type'](in_mlp))\n",
    "        elif self.config['conv_layer_type'].__name__ == 'GATConv':\n",
    "            self.graphConv_layer_list = torch.nn.ModuleList()\n",
    "            for layer_idx, layer in enumerate(self.config['layer_size_list']):\n",
    "                if layer_idx == 0:\n",
    "                    self.graphConv_layer_list.append(self.config['conv_layer_type'](*layer, heads = 8, bias = False, dropout = config['dropout']))\n",
    "                elif layer_idx < len(self.config['layer_size_list']) - 1:\n",
    "                    self.graphConv_layer_list.append(self.config['conv_layer_type'](in_channels = layer[0] * 8, out_channels = layer[1], heads = 8, bias = False, dropout = config['dropout']))\n",
    "                else:\n",
    "                    self.graphConv_layer_list.append(self.config['conv_layer_type'](in_channels = layer[0] * 8, out_channels = layer[1], heads = 1, bias = False, dropout = config['dropout']))\n",
    "        elif self.config['conv_layer_type'].__name__ == 'SAGEConv':\n",
    "            self.graphConv_layer_list = torch.nn.ModuleList([self.config['conv_layer_type'](*layer, aggr = config['pooling_option'].__name__.split('_')[1], bias = False) for layer in self.config['layer_size_list']])\n",
    "\n",
    "        if self.config['batch_normalization']:\n",
    "            self.bn_list = torch.nn.ModuleList([torch.nn.BatchNorm1d(layer[1]) for layer in self.config['layer_size_list']])\n",
    "\n",
    "        self.linear_layer_list = torch.nn.ModuleList([Linear(32, 32, bias = False), Linear(32, 32, bias = False)])\n",
    "        self.linear_bn_list = torch.nn.ModuleList([torch.nn.BatchNorm1d(32)])\n",
    "\n",
    "    def forward(self, x_data, edge_index_data, batch):\n",
    "        x, edge_index = x_data, edge_index_data\n",
    "\n",
    "        if self.config['virtual_edges']:\n",
    "            edge_index, _ = remove_self_loops(edge_index)\n",
    "            edge_index, _ = add_self_loops(edge_index, num_nodes = x.size(0))\n",
    "\n",
    "        num_layers = len(self.graphConv_layer_list)\n",
    "        for layer_idx, layer in enumerate(self.graphConv_layer_list):\n",
    "            x = layer(x, edge_index)\n",
    "            if self.config['conv_layer_type'].__name__ == 'GCNConv':\n",
    "                if self.config['batch_normalization']:\n",
    "                    x = self.bn_list[layer_idx](x)\n",
    "                x = self.config['activation_func'](x)\n",
    "                if self.config['dropout'] > 0.0 and self.training:\n",
    "                    x = F.dropout(x, p = self.config['dropout'], training = self.training)\n",
    "            elif self.config['conv_layer_type'].__name__ == 'GINConv':\n",
    "                if self.config['batch_normalization']:\n",
    "                    x = self.bn_list[layer_idx](x)\n",
    "                x = self.config['activation_func'](x)\n",
    "                if self.config['dropout'] > 0.0 and self.training: \n",
    "                    x = F.dropout(x, p = self.config['dropout'], training = self.training) \n",
    "            elif self.config['conv_layer_type'].__name__ == 'GATConv':\n",
    "                if self.config['batch_normalization']:\n",
    "                    x = self.bn_list[layer_idx](x)\n",
    "                x = self.config['activation_func'](x)\n",
    "                if self.config['dropout'] > 0.0 and self.training: \n",
    "                    x = F.dropout(x, p = self.config['dropout'], training = self.training)\n",
    "            elif self.config['conv_layer_type'].__name__ == 'SAGEConv':\n",
    "                if self.config['batch_normalization']:\n",
    "                    x = self.bn_list[layer_idx](x)\n",
    "                x = self.config['activation_func'](x)\n",
    "                if self.config['dropout'] > 0.0 and self.training: \n",
    "                    x = F.dropout(x, p = self.config['dropout'], training = self.training)               \n",
    "\n",
    "        x_pooled = self.config['pooling_option'](x, batch)\n",
    "\n",
    "        num_layers = len(self.linear_layer_list)\n",
    "        for layer_idx, layer in enumerate(self.linear_layer_list):\n",
    "            x_pooled = layer(x_pooled)\n",
    "            if layer_idx < num_layers - 1:\n",
    "                if self.config['batch_normalization']:\n",
    "                    x_pooled = self.linear_bn_list[layer_idx](x_pooled)\n",
    "                x_pooled = self.config['activation_func'](x_pooled)\n",
    "                if self.config['dropout'] > 0.0 and self.training:\n",
    "                    x_pooled = F.dropout(x_pooled, p = self.config['dropout'], training = self.training)           \n",
    "        \n",
    "        return x, x_pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config_str(config):\n",
    "    str_config = {}\n",
    "    for k, v in config.items():\n",
    "        if type(v).__name__ == 'function' or type(v).__name__ == 'type':\n",
    "            str_config[k] = v.__name__\n",
    "        elif type(v).__name__ == 'dict':\n",
    "            for k1, v1 in v.items():\n",
    "                if type(v1).__name__ == 'function' or type(v1).__name__ == 'type':\n",
    "                    str_config[k1] = v1.__name__\n",
    "                else:\n",
    "                    str_config[k1] = v1\n",
    "        else:\n",
    "            str_config[k] = v\n",
    "    str_config = str(str_config)\n",
    "\n",
    "    return str_config\n",
    "\n",
    "def get_model_setting(config):\n",
    "    layer_size_str = ''.join(str(s[-1])+'-' for s in config['layer_size_list'])[:-1]\n",
    "    conv_layer_type_str = config['conv_layer_type'].__name__\n",
    "    batch_size_str = str(config['batch_size'])\n",
    "    bn_str = 'batchNorm' if config['batch_normalization'] else 'noBatchNorm'\n",
    "    pooling_str = config['pooling_option'].__name__.split('_')[1] + 'Pool'\n",
    "    dropout_str = 'dropout' + str(config['dropout'])\n",
    "    lr_str = 'lr' + str(config['lr'])\n",
    "    nu_str = str(config['nu'])\n",
    "    wd_lamda_str = str(config['wd_lambda'])\n",
    "    eps_str = str(config['eps'])\n",
    "    loss_type_str = config['loss_type']\n",
    "    model_setting = f'batchSize{batch_size_str}_{conv_layer_type_str}{layer_size_str}_linear32-32_{pooling_str}_{bn_str}_{dropout_str}_{lr_str}_nu{nu_str}_wdL{wd_lamda_str}_eps{eps_str}_loss{loss_type_str}'\n",
    "    return model_setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_encoder(config):\n",
    "    save_addr_base = 'detector'\n",
    "    EPOCHS = 2\n",
    "    dataset_dict = split_dataset(train_dataset, test_dataset, adversarial_dataset, config['batch_size'])\n",
    "    model_setting, config_str = get_model_setting(config), get_config_str(config)\n",
    "    print(f'Model Setting is: {model_setting}')\n",
    "\n",
    "    error_dict, score_dist, detection_dict = {}, {}, {}\n",
    "\n",
    "    for normal_class in dataset_dict.keys():\n",
    "        if normal_class == 'adversarial':\n",
    "            continue\n",
    "        \n",
    "        print(f'<<<--- Normal Class is: {normal_class} --->>>')\n",
    "\n",
    "        model = Detector(config).to(device)\n",
    "        print(model)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr = config['lr'], weight_decay = config['wd_lambda'])\n",
    "\n",
    "        data_center = None\n",
    "        radius = torch.tensor(0, device = device)\n",
    "\n",
    "        '****** START Data Center Measurement ******'\n",
    "        print('Measuring Data Center.')\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for data_idx, data in enumerate(dataset_dict[normal_class]['train']):\n",
    "                data_gpu = data.to(device)\n",
    "                _, graph_embedding_batch = model(data_gpu.x, data_gpu.edge_index, data_gpu.batch)\n",
    "                if data_idx == 0:\n",
    "                    data_center = graph_embedding_batch\n",
    "                else:\n",
    "                    data_center = torch.cat((data_center, graph_embedding_batch), dim = 0)\n",
    "            data_center = data_center.mean(dim = 0)\n",
    "            data_center[(abs(data_center) < config['eps']) & (data_center < 0)] = -config['eps']\n",
    "            data_center[(abs(data_center) < config['eps']) & (data_center > 0)] = config['eps']\n",
    "        print('Done Measuring Data Center.')\n",
    "        '****** END Data Center Measurement ******'\n",
    "\n",
    "        error_dict[normal_class] = {'train': [], 'test': {'adversarial': {}}}\n",
    "        for k in dataset_dict.keys():\n",
    "            if k != 'adversarial':\n",
    "                error_dict[normal_class]['test'][k] = []\n",
    "            else:\n",
    "                for err_k in dataset_dict[k].keys():\n",
    "                    error_dict[normal_class]['test'][k][err_k] = []\n",
    "        print('init error dict is: ', error_dict)\n",
    "        score_dist[normal_class] = {}\n",
    "        detection_dict[normal_class] = {}\n",
    "        \n",
    "        for epoch in range(EPOCHS):\n",
    "            print(f'<<<<====== Epoch {epoch + 1} / {EPOCHS} ======>>>>')\n",
    "            score_dist[normal_class][epoch] = {'train': [], 'test': {k: [] for k in dataset_dict.keys()}}\n",
    "            score_dist[normal_class][epoch]['test']['adversarial'] = {k: [] for k in dataset_dict['adversarial'].keys()}\n",
    "\n",
    "\n",
    "            \"****** START Training Epoch ******\"\n",
    "            model.train()\n",
    "            epoch_loss, n_batches = 0, 0\n",
    "            for data_idx, data in enumerate(dataset_dict[normal_class]['train']):\n",
    "                model.zero_grad()\n",
    "                data_gpu = data.to(device)\n",
    "                _, graph_embedding_batch = model(data_gpu.x, data_gpu.edge_index, data_gpu.batch)\n",
    "                dist = torch.sum((graph_embedding_batch - data_center) ** 2, dim=1)\n",
    "\n",
    "                if config['loss_type'] == 'soft_boundary':\n",
    "                    scores = dist - radius ** 2\n",
    "                    loss = radius ** 2 + (1 / config['nu']) * torch.mean(torch.max(torch.zeros_like(scores), scores))\n",
    "                else:\n",
    "                    scores = dist\n",
    "                    loss = torch.mean(dist)\n",
    "\n",
    "                score_dist[normal_class][epoch]['train'] = scores.detach().to('cpu').numpy() if data_idx == 0 else np.concatenate((score_dist[normal_class][epoch]['train'], scores.detach().to('cpu').numpy()), axis = 0)                \n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                \"****** START Measuring R ******\"\n",
    "                if config['loss_type'] == 'soft_boundary':\n",
    "                    radius = np.quantile(np.sqrt(dist.detach().to('cpu').numpy()), 1 - config['nu'])\n",
    "                \"****** END Measuring R ******\"\n",
    "\n",
    "                n_batches += 1\n",
    "                epoch_loss += loss.item()\n",
    "            epoch_loss /= n_batches\n",
    "            error_dict[normal_class]['train'].append(epoch_loss)\n",
    "            score_dist[normal_class][epoch]['train'] = score_dist[normal_class][epoch]['train'].tolist()\n",
    "\n",
    "            plt.figure(figsize=(15, 5))\n",
    "            plt.plot(np.arange(1, epoch + 2, 1).tolist(), error_dict[normal_class]['train'])\n",
    "            plt.grid()\n",
    "            plt.title(f'Train Loss for {model_setting}')\n",
    "            plt.xticks(range(1, epoch + 2))\n",
    "            plt.savefig(f'{save_addr_base}/plots/{model_setting}_train_loss.pdf', bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \"****** END Training Epoch ******\"\n",
    "\n",
    "            \"****** START Testing Epoch ******\"\n",
    "            model.eval()\n",
    "            plt.figure(figsize=(15, 5))\n",
    "            for class_name in dataset_dict.keys():\n",
    "                epoch_loss, n_batches = 0, 0\n",
    "\n",
    "                if class_name != 'adversarial':\n",
    "                    loader = dataset_dict[class_name]['test']\n",
    "                    with torch.no_grad():\n",
    "                        for data_idx, data in enumerate(loader):\n",
    "                            data_gpu = data.to(device)\n",
    "                            _, graph_embedding_batch = model(data_gpu.x, data_gpu.edge_index, data_gpu.batch)\n",
    "\n",
    "                            dist = torch.sum((graph_embedding_batch - data_center) ** 2, dim=1)\n",
    "\n",
    "                            if config['loss_type'] == 'soft_boundary':\n",
    "                                scores = dist - radius ** 2\n",
    "                                loss = radius ** 2 + (1 / config['nu']) * torch.mean(torch.max(torch.zeros_like(scores), scores))\n",
    "                            else:\n",
    "                                scores = dist\n",
    "                                loss = torch.mean(dist)\n",
    "                        \n",
    "                            score_dist[normal_class][epoch]['test'][class_name] = scores.detach().to('cpu').numpy() if data_idx == 0 else np.concatenate((score_dist[normal_class][epoch]['test'][class_name], scores.detach().to('cpu').numpy()), axis = 0)\n",
    "            \n",
    "                            n_batches += 1\n",
    "                            epoch_loss += loss.item()\n",
    "                        epoch_loss /= n_batches\n",
    "                        error_dict[normal_class]['test'][class_name].append(epoch_loss)\n",
    "                        score_dist[normal_class][epoch]['test'][class_name] = score_dist[normal_class][epoch]['test'][class_name].tolist()\n",
    "            \n",
    "                    plt.plot(np.arange(1, epoch + 2, 1).tolist(), error_dict[normal_class]['test'][class_name], label = class_name)\n",
    "\n",
    "                if class_name == 'adversarial':\n",
    "                    for sub_adv_k in dataset_dict['adversarial'].keys():\n",
    "                        loader = dataset_dict['adversarial'][sub_adv_k]\n",
    "                        with torch.no_grad():\n",
    "                            for data_idx, data in enumerate(loader):\n",
    "                                data_gpu = data.to(device)\n",
    "                                _, graph_embedding_batch = model(data_gpu.x, data_gpu.edge_index, data_gpu.batch)\n",
    "\n",
    "                                dist = torch.sum((graph_embedding_batch - data_center) ** 2, dim=1)\n",
    "\n",
    "                                if config['loss_type'] == 'soft_boundary':\n",
    "                                    scores = dist - radius ** 2\n",
    "                                    loss = radius ** 2 + (1 / config['nu']) * torch.mean(torch.max(torch.zeros_like(scores), scores))\n",
    "                                else:\n",
    "                                    scores = dist\n",
    "                                    loss = torch.mean(dist)\n",
    "                            \n",
    "                                score_dist[normal_class][epoch]['test'][class_name][sub_adv_k] = \\\n",
    "                                    scores.detach().to('cpu').numpy() if data_idx == 0 else np.concatenate((score_dist[normal_class][epoch]['test'][class_name][sub_adv_k], scores.detach().to('cpu').numpy()), axis = 0)\n",
    "                \n",
    "                                n_batches += 1\n",
    "                                epoch_loss += loss.item()\n",
    "                            epoch_loss /= n_batches\n",
    "                            error_dict[normal_class]['test'][class_name][sub_adv_k].append(epoch_loss)\n",
    "                            score_dist[normal_class][epoch]['test'][class_name][sub_adv_k] = score_dist[normal_class][epoch]['test'][class_name][sub_adv_k].tolist()\n",
    "                \n",
    "                        plt.plot(np.arange(1, epoch + 2, 1).tolist(), error_dict[normal_class]['test'][class_name][sub_adv_k], label = sub_adv_k)\n",
    "            plt.grid()\n",
    "            plt.title(f'Test Loss for {model_setting}')\n",
    "            plt.legend()\n",
    "            plt.xticks(range(1, epoch + 2))\n",
    "            plt.savefig(f'{save_addr_base}/plots/{model_setting}_test_loss.pdf', bbox_inches='tight')\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "            print()\n",
    "\n",
    "\n",
    "            for score_k, score_v in score_dist[normal_class][epoch]['test'].items():\n",
    "                if score_k != 'adversarial':\n",
    "                    print(f'Number of Scores for {score_k} Class: ', len(score_v), end = ' ')\n",
    "                else:\n",
    "                    for xx, yy in score_v.items():\n",
    "                        print(f'Number of Scores for {xx} Class: ', len(yy), end = ' ')\n",
    "            print()\n",
    "    \n",
    "            if config['loss_type'] == 'soft_boundary':\n",
    "                normal_passed = np.count_nonzero(np.array(score_dist[normal_class][epoch]['test'][normal_class]) < 0)\n",
    "                detection_dict[normal_class][epoch] = {'passed': normal_passed / len(score_dist[normal_class][epoch]['test'][normal_class]), 'detected': {}}\n",
    "                print(f\"# {normal_class} Samples Passed: {normal_passed / len(score_dist[normal_class][epoch]['test'][normal_class])} with a soft threshold of 0\")\n",
    "\n",
    "                for xx, yy in score_dist[normal_class][epoch]['test']['adversarial'].items():\n",
    "                    adv_detected = np.count_nonzero(np.array(yy) > 0)\n",
    "                    print(f\"# {xx} Samples Detected: {adv_detected / len(yy)} with a soft threshold of 0\")\n",
    "                    detection_dict[normal_class][epoch]['detected'][xx] =  adv_detected / len(yy)\n",
    "            elif config['loss_type'] == 'hard_boundary':\n",
    "                prc_threshold = 95.9\n",
    "                normal_pcn = np.percentile(score_dist[normal_class][epoch]['test'][normal_class], prc_threshold)\n",
    "                detection_dict[normal_class][epoch] = {'passed': prc_threshold * 0.01, 'detected': {}}\n",
    "\n",
    "                for xx, yy in score_dist[normal_class][epoch]['test']['adversarial'].items():\n",
    "                    adv_detected = np.count_nonzero(np.array(yy) > normal_pcn)\n",
    "                    print(f\"# {xx} Samples Detected: {adv_detected / len(yy)} with a hard threshold of {normal_pcn}\")\n",
    "                    detection_dict[normal_class][epoch]['detected'][xx] =  adv_detected / len(yy)\n",
    "\n",
    " \n",
    "            fig, ax = plt.subplots()\n",
    "            fig.set_figheight(3);fig.set_figwidth(10)\n",
    "            ax.set_title(f'The score Distribution with Normal Class: {normal_class}')\n",
    "            plot_class_info = {}\n",
    "            for score_k, score_v in score_dist[normal_class][epoch]['test'].items():\n",
    "                if score_k != 'adversarial':\n",
    "                    plot_class_info[score_k] = score_v\n",
    "                else:\n",
    "                    for adv_score_k, adv_score_v in score_v.items():\n",
    "                        plot_class_info[adv_score_k] = adv_score_v\n",
    "            ax.boxplot(plot_class_info.values())\n",
    "            ax.set_xticklabels(plot_class_info.keys())\n",
    "            ax.grid()\n",
    "            plt.show()\n",
    "            del plot_class_info\n",
    "            \"****** END Testing Epoch ******\"\n",
    "\n",
    "        \n",
    "        with open(save_addr_base + f'/logs/' + f'score_dist_{model_setting}.json', 'w') as f:\n",
    "            json.dump(score_dist[normal_class], f)\n",
    "        with open(save_addr_base + f'/logs/' + f'error_{model_setting}.json', 'w') as f:\n",
    "            json.dump(error_dict[normal_class], f)\n",
    "        with open(save_addr_base + f'/logs/' + f'detection_{model_setting}.json', 'w') as f:\n",
    "            json.dump(detection_dict[normal_class], f)\n",
    "\n",
    "        clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for config in [\n",
    "  {'nu': 0.1, 'wd_lambda': 5e-4, 'eps': 0.001, 'lr': 0.001, 'loss_type': 'hard_boundary', 'batch_size': 128, 'dropout': 0.5, 'optimizer': torch.optim.Adam, 'virtual_edges': False, 'conv_layer_type': SAGEConv, 'layer_size_list': [(188, 64), (64, 64), (64, 32)], 'conv_normalize': False, 'batch_normalization': False, 'pooling_option': global_add_pool, 'activation_func': F.relu},\n",
    "  ]:\n",
    "  \n",
    "  train_test_encoder(config)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a563c4fea09d6faebdb5e5ac3287d5ca559a69631ad74c32f34398c0910a8f01"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('PyG')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
